\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}MLP backprop and NumPy implementation}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Analytical derivation of gradients}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1\tmspace  +\thinmuskip {.1667em}a)}}{1}{subsubsection.1.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1\tmspace  +\thinmuskip {.1667em}b)}}{2}{subsubsection.1.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1\tmspace  +\thinmuskip {.1667em}c)}}{3}{subsubsection.1.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}NumPy implementation}{4}{subsection.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}PyTorch MLP}{4}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Custom Module: Batch Normalization}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Automatic differentiation}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Manual implementation of backward pass}{4}{subsection.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}PyTorch CNN}{4}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training and test accuracies and losses for the MLP model. The test accuracies and losses are produced every 500 steps. The final test accuracy was $55.7 \%$\relax }}{5}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mlp_results}{{1}{5}{Training and test accuracies and losses for the MLP model. The test accuracies and losses are produced every 500 steps. The final test accuracy was $55.7 \%$\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training and test accuracies and losses for the MLP model using batch normalization. The test accuracies and losses are produced every 500 steps. The final test accuracy was $57.1 \%$\relax }}{6}{figure.caption.2}}
\newlabel{fig:mlp_batchnorm_results}{{2}{6}{Training and test accuracies and losses for the MLP model using batch normalization. The test accuracies and losses are produced every 500 steps. The final test accuracy was $57.1 \%$\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training and test accuracies and losses for the ConvNet model. The test accuracies and losses are produced every 500 steps. The final test accuracy was $77.3\%$\relax }}{6}{figure.caption.3}}
\newlabel{fig:convnet_results}{{3}{6}{Training and test accuracies and losses for the ConvNet model. The test accuracies and losses are produced every 500 steps. The final test accuracy was $77.3\%$\relax }{figure.caption.3}{}}
